{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e9013",
   "metadata": {},
   "source": [
    "### Setup Langchain + LLM\n",
    "1. Install Langchain: \n",
    "- pip intall langchain\n",
    "2. Install integration packages.\n",
    "- pip install -U langchain-cohere\n",
    "- pip install -U langchain-groq\n",
    "- pip install -U langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae4cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help you with that. \\n\\nCould you please provide me with your location so I can give you the most accurate weather information? \\n\\n- Have a great day' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '3ad3e21d-9808-4f80-beeb-02fbea59d8c6', 'token_count': {'input_tokens': 232.0, 'output_tokens': 38.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '3ad3e21d-9808-4f80-beeb-02fbea59d8c6', 'token_count': {'input_tokens': 232.0, 'output_tokens': 38.0}} id='run-5132eddc-2f66-41f8-9041-d52171175fde-0' usage_metadata={'input_tokens': 232, 'output_tokens': 38, 'total_tokens': 270}\n",
      "content=\"I'm happy to help! As of now, it's a beautiful day out there! According to our latest forecast, the sun is shining brightly with a clear blue sky and a gentle breeze blowing at about 5 mph. The temperature is currently a pleasant 72°F (22°C) with a high of 78°F (25°C) expected later today.\\n\\nAs for the rest of the week, we're expecting a mix of sunshine and scattered clouds, with a slight chance of showers on Wednesday and Thursday. Temperatures will remain mild, ranging from the mid-60s to the mid-70s (18-23°C).\\n\\nOf course, always check our website or mobile app for the latest updates and any potential changes in the forecast. And don't forget to pack that sunscreen for your outdoor activities!\\n\\nHave a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 169, 'prompt_tokens': 51, 'total_tokens': 220, 'completion_time': 0.140833333, 'prompt_time': 0.006369094, 'queue_time': 0.008259845000000002, 'total_time': 0.147202427}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-63899a82-91a6-4ec2-bfc2-927138fd4027-0' usage_metadata={'input_tokens': 51, 'output_tokens': 169, 'total_tokens': 220}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "groq = config['groq']\n",
    "cohere = config['cohere']\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "os.environ['COHERE_API_KEY'] = cohere.get('COHERE_API_KEY')\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02039a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Modeling in Large Language Models (LLMs) is a fascinating aspect of natural language processing and machine learning. At its core, modeling in LLMs involves creating mathematical models that can understand, interpret, and generate human language.\\n\\nHere\\'s a simplified breakdown of the process:\\n\\n1. **Training Data**: LLMs are typically trained on massive amounts of text data. This data can come from books, articles, websites, social media, and any other source of human language. The quality and diversity of the training data are crucial for the model\\'s performance.\\n\\n2. **Tokenization**: Before feeding the text data into the model, it needs to be tokenized. Tokenization is the process of breaking down the text into smaller pieces, such as words or subwords. These tokens become the basic units that the model works with.\\n\\n3. **Model Architecture**: The choice of model architecture depends on the specific task and requirements. Recurrent Neural Networks (RNNs), Transformer models (such as GPT), and Transformer variants are commonly used architectures for LLMs. These architectures are designed to capture the contextual relationships and patterns in the language data.\\n\\n4. **Model Training**: During training, the model learns from the tokenized text data by adjusting its internal parameters. This is typically done using a technique called \"gradient descent,\" where the model iteratively updates its parameters to minimize the difference between predicted outputs and actual outputs. The training process can take a significant amount of time and computational resources due to the large volume of data and complex model architectures.\\n\\n5. **Language Understanding**: Through training, the model learns to understand language by recognizing patterns, relationships, and context. It can learn grammar, syntax, semantics, and even nuances of language usage. This enables the model to interpret and generate human-like text.\\n\\n6. **Evaluation and Fine-Tuning**: After training, the model\\'s performance is evaluated using various metrics, such as accuracy, precision, recall, and F1 score. If the model does not meet the desired performance, fine-tuning can be applied. Fine-tuning involves further training the model on a smaller, task-specific dataset to improve its performance on a particular task.\\n\\n7. **Deployment**: Once the model is trained and fine-tuned, it can be deployed to perform various language-related tasks, such as language generation, text classification, machine translation, question answering, and more.\\n\\nModeling in LLMs has advanced significantly in recent years, leading to models that can generate human-like text, answer complex questions, and perform a wide range of language-related tasks. However, it\\'s important to note that there are also challenges and limitations, such as the need for vast amounts of data, potential biases in the data, ethical considerations, and the difficulty of evaluating and interpreting model outputs.\\n\\nHave a great day!' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '650a946e-72e3-430a-9316-fd5815db142f', 'token_count': {'input_tokens': 239.0, 'output_tokens': 581.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '650a946e-72e3-430a-9316-fd5815db142f', 'token_count': {'input_tokens': 239.0, 'output_tokens': 581.0}} id='run-c944d5d1-7acb-4418-8593-2373f90e6c11-0' usage_metadata={'input_tokens': 239, 'output_tokens': 581, 'total_tokens': 820}\n"
     ]
    }
   ],
   "source": [
    "messages=[\n",
    "    SystemMessage(content='You are a general helping service. You will respond to general queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='tell me about modeling in llm ?')\n",
    "]\n",
    "\n",
    "model=ChatCohere(model=\"command-r-plus\")\n",
    "# try human message tamplete use fstring\n",
    "print(model.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fa0d3-c23a-455f-998f-bb68018d0db0",
   "metadata": {},
   "source": [
    "## Create the prompt\n",
    "1. Imports Human and System message classes. System represents our instructions to GPT and Human represents the question or prompt that the user provides.\n",
    "2. LangChain responses are instances of class `BaseMessage` It contains the actual response from GPT and some other metadata.\n",
    "3. Since we are interested only in the string reponse that GPT gave we chain (pipe) the reponse to a parser\n",
    "4. For our purpose we will use `StrOutputParser` class\n",
    "5. Next we create a `chain` using the components `model` and `parser`\n",
    "6. Finally we call the `invoke` method on the chain and pass our `messages` list to it.\n",
    "7. In the output cell we get the response from `GPT-35-turbo`\n",
    "\n",
    "*A chain is an wrapper around multiple individual components that are executed in a defined order. Components in LangChain implement `Runnable` interface. This interface have a method `invoke` that transforms single input to an output.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f9fb1d-d604-41ab-8e62-5ea4e6a9213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<cohere.client.Client object at 0x729228637530> async_client=<cohere.client.AsyncClient object at 0x72922846bb00> model='command-r' cohere_api_key=SecretStr('**********')\n",
      "['/usr/local/python/3.12.1/lib/python312.zip', '/usr/local/python/3.12.1/lib/python3.12', '/usr/local/python/3.12.1/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/usr/local/python/3.12.1/lib/python3.12/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A classic lateral thinking puzzle!\\n\\nThe answer is not to choose a room at all. Instead, the man should choose not to accept his punishment and appeal his sentence. After all, he's been condemned to death, but that doesn't mean he has to actually die!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The classes used for setting up the prompt\n",
    "import puzzles\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate #import the Class for creating a prompt\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "puzzle = puzzles.puzzles('hungryLions') # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"brief\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d08a9111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A classic puzzle!\\n\\nHere's the solution:\\n\\n1. Turn switch 1 to ON for 5 minutes.\\n2. Turn switch 1 to OFF, and turn switch 2 to ON.\\n3. Turn switch 2 to OFF, and turn switch 3 to ON.\\n4. Turn switch 3 to OFF.\\n5. Open the door and observe the bulbs.\\n\\nNow, let's analyze the situation:\\n\\n* Bulb 1 was turned ON for 5 minutes, so it will be warm.\\n* Bulb 2 was turned ON after bulb 1 was turned OFF, so it will be hot.\\n* Bulb 3 was turned ON after both bulb 1 and 2 were turned OFF, so it will be cold.\\n\\nBy observing the bulbs, you can determine which switch corresponds to which bulb:\\n\\n* Switch 1 corresponds to Bulb 1 (warm).\\n* Switch 2 corresponds to Bulb 2 (hot).\\n* Switch 3 corresponds to Bulb 3 (cold).\\n\\nYou've successfully identified each switch with respect to its bulb without opening the door more than once!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser=StrOutputParser()\n",
    "\n",
    "puzzle=puzzles.puzzles('3 Bulbs and 3 Switches')\n",
    "\n",
    "system_template = \"solve the following puzzle. please provide a {responseType} responce.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "chain.invoke({\"responseType\":\"brief\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498ce4c",
   "metadata": {},
   "source": [
    "### Chatbot \n",
    "1. We begin with creating a basic chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a61142d-54d4-46b8-a50a-3d1b473e82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "I'm happy to help! However, I'm a large language model, I don't have the ability to know your personal information, including your name. Each time you interact with me, it's a new conversation, and I don't retain any information from previous conversations.\n",
      "\n",
      "If you'd like to share your name with me, I'm happy to learn it and use it in our conversation. Otherwise, I can simply address you as \"user\" or \"friend\" if you prefer!\n"
     ]
    }
   ],
   "source": [
    "chain = model | parser\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=\"hi I am Bob\")])\n",
    "\n",
    "print(response)\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=\"what is my name\")])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ff0e7",
   "metadata": {},
   "source": [
    "#### Lets dig into what is happening here.\n",
    "1. Click here to check the UML diagram: \n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#56cf\n",
    "\n",
    "\n",
    "#### Runnable\n",
    "1. Its an extremely prominent class and used extensively in creating chains.\n",
    "2. Chains combine components together in a pipeline\n",
    "3. Many components like all models, parsers, prompts and anything that can logically go into a chain derives from it.\n",
    "4. `ChatGroq` is provided partner by extends `BaseChatModel` from langchain_core\n",
    "5. https://github.com/langchain-ai/langchain/blob/master/libs/partners/groq/langchain_groq/chat_models.py\n",
    "6. This is the base class for all model classes offered by any partner.\n",
    "7. `BaseClass` extends `RunnableSerializable` that supports serialization into JSON\n",
    "8. `RunnableSerializable` extends `Runnable` that means it can participate in chains.\n",
    "9. You can also use `RunnableSequence` to construct the chain.\n",
    "10. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L2659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85ecfeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"hi i am bob\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a49d5c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of 2023, the President of the United States is Joe Biden. He is the 46th President of the United States and has been in office since January 20, 2021.\\n\\nHere is a list of all the Presidents of the United States in chronological order:\\n\\n1. George Washington (1789-1797)\\n2. John Adams (1797-1801)\\n3. Thomas Jefferson (1801-1809)\\n4. James Madison (1809-1817)\\n5. James Monroe (1817-1825)\\n6. John Quincy Adams (1825-1829)\\n7. Andrew Jackson (1829-1837)\\n8. Martin Van Buren (1837-1841)\\n9. William Henry Harrison (1841-1841)\\n10. John Tyler (1841-1845)\\n11. James K. Polk (1845-1849)\\n12. Zachary Taylor (1849-1850)\\n13. Millard Fillmore (1850-1853)\\n14. Franklin Pierce (1853-1857)\\n15. James Buchanan (1857-1861)\\n16. Abraham Lincoln (1861-1865)\\n17. Andrew Johnson (1865-1869)\\n18. Ulysses S. Grant (1869-1877)\\n19. Rutherford B. Hayes (1877-1881)\\n20. James A. Garfield (1881-1881)\\n21. Chester A. Arthur (1881-1885)\\n22. Grover Cleveland (1885-1889)\\n23. Benjamin Harrison (1889-1893)\\n24. Grover Cleveland (1893-1897)\\n25. William McKinley (1897-1901)\\n26. Theodore Roosevelt (1901-1909)\\n27. William Howard Taft (1909-1913)\\n28. Woodrow Wilson (1913-1921)\\n29. Warren G. Harding (1921-1923)\\n30. Calvin Coolidge (1923-1929)\\n31. Herbert Hoover (1929-1933)\\n32. Franklin D. Roosevelt (1933-1945)\\n33. Harry S. Truman (1945-1953)\\n34. Dwight D. Eisenhower (1953-1961)\\n35. John F. Kennedy (1961-1963)\\n36. Lyndon B. Johnson (1963-1969)\\n37. Richard Nixon (1969-1974)\\n38. Gerald R. Ford (1974-1977)\\n39. Jimmy Carter (1977-1981)\\n40. Ronald Reagan (1981-1989)\\n41. George H.W. Bush (1989-1993)\\n42. Bill Clinton (1993-2001)\\n43. George W. Bush (2001-2009)\\n44. Barack Obama (2009-2017)\\n45. Donald J. Trump (2017-2021)\\n46. Joe Biden (2021-present)\\n\\nNote: There have been a total of 46 presidencies, but only 45 different individuals have held the office, as Grover Cleveland served two non-consecutive terms (#22 and #24) and is therefore counted as the 22nd and 24th President.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model=ChatCohere(model=\"command-r-plus\")\n",
    "chain = RunnableSequence(model,parser)\n",
    "chain.invoke([HumanMessage(content=\"who is precident of america\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c363b",
   "metadata": {},
   "source": [
    "1. Chain calls the first component and passes any arguments provided to it.\n",
    "2. In this case its an object of type `HumanMessage`\n",
    "3. This is how a chain looks: https://miro.medium.com/v2/resize:fit:750/format:webp/1*K1F-m4gImEUO0AELkpQuKg.jpeg\n",
    "4. Each model component by any partner provides an object of type `BaseMessage`. This is then passed to the next component.\n",
    "5. This is the signature of invoke of a model class\n",
    "\n",
    "`def` `invoke(str | List[dict | tuple | BaseMessage] | PromptValue):`\\\n",
    "    Suite\n",
    "  \n",
    "6. In our example `HumanMessage` is derived from `BaseMessage` which needs `content` for initialization.\n",
    "\n",
    "`param content: Union[str, List[Union[str,Dict]]]`\n",
    "\n",
    "7. Union, List, Dict are all defined in typing module\n",
    "8. Union means one of the input types is expected. We are passing a string.\n",
    "\n",
    "9. Our `parser` is of type `StrOutputParser` that extends `BaseOutputParser`\n",
    "10. Its invoke is:\n",
    "\n",
    "`def invoke(self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None) -> T:`\n",
    "\n",
    "11.  This says input can be either string or `BaseMessage`. We are using `BaseMessage` the return type of `model`\n",
    "\n",
    "12. Some useful methods are:\n",
    "- parser.input_schema.schema() # get JSON schema of the input\n",
    "- parser.output_schema.schema() # gets JSON schema of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9b92",
   "metadata": {},
   "source": [
    "### Adding history to chat\n",
    "1. At this stage if you pass another message to the model it will have no recollection of the earlier message.\n",
    "2. Lets add history. Chat history is managed by a set of classes offered by community.\n",
    "3. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/chat_history.py\n",
    "4. `asyncio` is a Python library: https://docs.python.org/3/library/asyncio.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c94cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='Hi! I am Bob', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you know my name dude?', additional_kwargs={}, response_metadata={})]\n",
      "I think I do! You said your name is Bob, right?\n"
     ]
    }
   ],
   "source": [
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage('Hi! I am Bob'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages) # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content) # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you know my name dude?'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0282e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "That's correct! New Delhi is the capital city of India.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\"InMemoryChatMessageHistory\" object has no field \"add_message\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minvoke(messages)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_message\u001b[49m\u001b[38;5;241m=\u001b[39m(SystemMessage(response\u001b[38;5;241m.\u001b[39mcontent))\n\u001b[1;32m      7\u001b[0m store\u001b[38;5;241m.\u001b[39madd_messsage\u001b[38;5;241m=\u001b[39m(HumanMessage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi visited New Delhi\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m(getMessage())\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic/main.py:884\u001b[0m, in \u001b[0;36mBaseModel.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_assignment(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_fields:\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;66;03m# TODO - matching error\u001b[39;00m\n\u001b[0;32m--> 884\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object has no field \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_fields:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_extra \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_extra:\n",
      "\u001b[0;31mValueError\u001b[0m: \"InMemoryChatMessageHistory\" object has no field \"add_message\""
     ]
    }
   ],
   "source": [
    "store=InMemoryChatMessageHistory()\n",
    "store.add_message(HumanMessage('New Delhi is the capital of india'))\n",
    "messages=await(getMessage())\n",
    "response = model.invoke(messages)\n",
    "print(response.content)\n",
    "store.add_message=(SystemMessage(response.content))\n",
    "store.add_messsage=(HumanMessage(\"i visited New Delhi\"))\n",
    "messages=await(getMessage())\n",
    "print(messages)\n",
    "response=model.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e221d9",
   "metadata": {},
   "source": [
    "1. There are some issues here. Since Chat History is not a descendant of Runnable we cannot chain it.\n",
    "2. Therefore the code is sort of littered. \n",
    "3. Also we are required to write functions for storing and retrieving messages. This should be rather standard and done by the framework!\n",
    "4. What about sessions? This code is running of the server which supports multiple users. So there needs to be a mechanism to manage sessions.\n",
    "\n",
    "#### RunnableWithMessageHistory\n",
    "1. This is where LangChain offers this class.\n",
    "2. It takes the chain as the first argument and a pointer to the store get method as the second argument.\n",
    "3. This class then takes the ownership of executing the chain and any component that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "949191e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! Nice to meet you! Is there something I can help you with or would you like to chat about something in particular?\n",
      "You told me your name is Bob! Am I right?\n"
     ]
    }
   ],
   "source": [
    "# Lets create our own store. This store will be a dict with a key for each session\n",
    "# The value for each key will be InMemoryChatHistory object \n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Bob\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ab1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable':{\"session_id\": 1001} }\n",
    "chatHistory = RunnableWithMessageHistory(model,get_session_history)\n",
    "response=chatHistory.invoke([HumanMessage(content=\"who is precident of india\")], config=config)\n",
    "print(response.content)\n",
    "response = chatHistory.invoke(\n",
    "    [HumanMessage(content=\"\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c080dd6",
   "metadata": {},
   "source": [
    "1. Here is a flowchart of this program.\n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#3c92\n",
    "3. Wrapper around another runnable - the chain\n",
    "4. https://techblogs.cloudlex.com/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#a0cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d9303",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
